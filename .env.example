# AI Provider Configuration
# Select one of: openai, anthropic, ollama, github_models
AI_PROVIDER=openai

# API Key for the selected provider
# OpenAI: Your OpenAI API key (https://platform.openai.com/api-keys)
# Anthropic: Your Anthropic API key (https://console.anthropic.com/)
# GitHub Models: Your GitHub PAT token with model inference permissions
# Ollama: Not required (leave empty for local Ollama)
AI_API_KEY=your_api_key_here

# Model name for the selected provider
# OpenAI: gpt-4o-mini, gpt-4o, gpt-4-turbo (default: gpt-4o-mini)
# Anthropic: claude-3-5-haiku, claude-3-5-sonnet, claude-3-opus (default: claude-3-5-haiku)
# Ollama: mistral, llama2, neural-chat, etc. (default: mistral)
# GitHub: gpt-4o-mini, gpt-4o, claude-3-5-haiku, etc.
AI_MODEL=gpt-4o-mini

# Optional: Custom base URL for API endpoint
# Ollama: http://localhost:11434 (default)
# OpenAI: https://api.openai.com (default)
# Anthropic: https://api.anthropic.com (default)
AI_BASE_URL=

# Optional: Temperature for model responses (0.0 - 2.0)
# Lower = more deterministic, Higher = more creative
AI_TEMPERATURE=0.7

# Optional: Maximum tokens in response
AI_MAX_TOKENS=2048

# Legacy configuration (kept for backward compatibility)
# GitHub Model Configuration (Free tier)
GITHUB_TOKEN=your_github_pat_here
MODEL_PROVIDER=github
MODEL_ID=openai/gpt-4o-mini

# Or use Microsoft Foundry (formerly Azure AI Foundry)
# FOUNDRY_ENDPOINT=https://your-project.api.azureml.ms
# FOUNDRY_KEY=your_foundry_key_here
# MODEL_PROVIDER=foundry
# MODEL_ID=gpt-4o

# Or use OpenAI
# OPENAI_API_KEY=your_openai_api_key_here
# MODEL_PROVIDER=openai
# MODEL_ID=gpt-4o-mini
